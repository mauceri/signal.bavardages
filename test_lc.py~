from langchain.llms import LlamaCpp
from langchain.prompts import ChatPromptTemplate
from langchain import PromptTemplate, LLMChain
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# Callbacks support token-wise streaming
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

# Make sure the model path is correct for your system!
llm = LlamaCpp(
#    model_path="/Users/mauceric/PRG/llama.cpp/models/7B/ggml-model-q4_0.bin",
    model_path="/Users/mauceric/PRG/llama.cpp/models/7B/gguf-model-q4_0.bin",
    temperature=1,
    max_tokens=500,
    top_p=1,
    callback_manager=callback_manager, 
    verbose=True, # Verbose is required to pass to the callback manager
)

prompt = ChatPromptTemplate.from_messages([("""
Vous parlez français et vous êtes concis dans vos réponses.
Q: {q}
"""
)])

ret=llm(prompt.format_messages(q="comment allez-vous ?")
print(ret)
